################################################################################
#
#   Run the IOR benchmark with the HDF5 I/O API and individual datasets per task
#
################################################################################
IOR START

### You MUST change the following parameters (see README.md)
    numTasks=104            # number of MPI processes to use. You may choose to use one or more MPI processes per node.
    segmentCount=177       # must be > fileSize / (blockSize * numTasks); fileSize should be >1.5x aggregate DRAM available for page cache
    memoryPerNode=80%       # must be > 80% of the node's DRAM available for use by the page cache

### You MAY change the following parameters
    transferSize=4M         # size of a single data buffer to be transferred by a single I/O call; tune for your storage system
    blockSize=4M            # must be the same as transferSize
    testFile=datafile.h5    # will read/write to a shared HDF5 file called datafile.h5
    collective=1            # for HDF5, use collective I/O if supported
    keepFile=1              # do not keep files used by the test at the end of each execution

### You MUST NOT change the following parameters
    reorderTasksConstant=1  # each node n writes data; that data is then read by node n+1
    intraTestBarriers=1     # use barriers between open/read/write/close
    repetitions=1           # executes the same test multiple times
    verbose=2               # print additional information about the job geometry
    fsync=1                 # for POSIX api call fsync(2) before close(2)

### The following parameters define the nature of the benchmark test
    api=HDF5                # use the HDF5 I/O API
    filePerProc=0           # read/write one shared file for all MPI processes
    randomOffset=0          # randomize order in which reads/writes occur
    writeFile=1             # perform the write component of the test
    readFile=1              # perform the read component of the test after the write component has completed
    individualDataSets=1    # each MPI task writes to its own dataset in the HDF5 file

RUN

IOR STOP
